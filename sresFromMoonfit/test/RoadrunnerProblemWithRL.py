import numpy as np
import tellurium as te
from sresFromMoonfit import SRES
from tellurium.roadrunner.extended_roadrunner import ExtendedRoadRunner

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl

import os

mpl.use("TkAgg")


###################################################################################3
#   highjacking telluriums interface to roadrunner
#

def freeParameters(self):
    """
    Roadrunner models do not have a concept of "free" or "fixed"
    parameters (maybe it should?). Either way, we add a cheeky method
    to the tellurium interface to roadrunner to return the names
    of the parameters we want to fit
    """
    return ["k1", "k2", "k3"]


# assign, and now freeParameters is callable bound to ExtendedRoadrunner types
ExtendedRoadRunner.freeParameters = freeParameters

#####################################################################################3
#   model and data generation
#

# globally scoped model
r = te.loada("""
   S1 -> S2; k1*S1;
   S2 -> S3; k2*S2;
   S3 -> S1; k3*S3;
   S1 = 1; S2 = 0; S3 = 0;
   k1 = 0.15; k2 = 0.45; k3 = 0.05;
""")


def get_data(**params):
    r.reset()
    for k, v in params.items():
        setattr(r, k, v)

    # Simulate "experimental" data
    m = r.simulate(0, 20, 21)
    x_data = m["time"]  # only time
    y_data = m[:, 1:]  # remove time

    return x_data, y_data, r.timeCourseSelections[1:]


x_data, y_data, selections = get_data(k1=0.15, k2=0.45, k3=0.05)


# Add random noise to synthetic data
# for i in range(0, len(y_data)):
#     t = np.random.normal(0, 0.05)
#     y_data[i] = y_data[i] + t


##############################################################
#   Configure the optimization
#

@SRES.COST_FUNCTION_CALLBACK
def cost_fun(parameters, fitness, constraints):
    """
    Brief
    -----
    Compute difference between experimental dataset and model simulation with candidate parameters.
    This cost function is user defined and used as input to the main SRES algorithm. The input
    to this function is always [parameters, fitness and constraints]. You do not need to worry
    about generating candidate parameters as they are generated by the underlying algorithm. You do
    however have to worry about updating the fitness value, which you do like this:

        fitness.contents.value = calculated_cost

    Where calculated_cost is a float computed by your function. Note, that even though
    we haven't used the constraints argument in this cost function, we still need to pass it in
    as an input parameter.

    Details
    -------
    The underlying SRES C code requires as input a function pointer to a cost function
    that has the following signature:

        typedef void(*ESfcnFG)(double *, double *, double *);

    We can create a cost function in Python to pass to C by using the
    :py:class:`SRES.COST_FUNCTION_CALLBACK` decorator. Since the C end is
    expecting a function with three double pointer types, we must have
    as arguments to our cost function, three arguments.

    When coding the cost function, you need to remember that the types of
    parameter, fitness and constraints are ctypes pointers to double
    arrays in the case of the parameter and constraints argument and
    a pointer to a double in the case of fitness. To do computation
    with these types you need the value that the pointer points to, not
    the pointer. To get these, you use:
        >>> parameters.contents[0]
    In the case of pointer to a double array or
        >>> fitness.contents.value
    in the case of a pointer to a double.

    Args
    ----
    parameters: A list of candidate parameters with the same size as the
                dimensionality of your defined optimization problem.
    fitness:    This is the value that you must compute and assign.

    """
    x, y, sel = get_data(
        **dict(
            zip(r.freeParameters(), parameters.contents))
    )
    # note, you'll need two np.sums when >1 dataset
    cost = np.sum(np.sum((y - y_data) ** 2))

    # update fitness. This step is crucial and evey cost function
    # must have this line
    fitness.contents.value = cost


# sres = SRES(
#     cost_function=cost_fun,
#     ngen=20,
#     lb=[0.01] * 3,
#     ub=[10] * 3,
#     parent_popsize=10,
#     child_popsize=750,
#     retry=100,
#
# )
import tensorflow as tf
import tensorflow.keras as keras


# make these things optimal for the current problem
# Action shape:
#   - parent_popsize
#   - child_popsize
#   - gamma
#   - alpha
#   - retry

# State shape (input):
#   - 1/best value
#   - Maybe input parameters too?


def get_qs(model, state, step):
    return model.predict(state.reshape([1, state.shape[0]]))[0]


def train(env, replay_memory, model, target_model, done):
    learning_rate = 0.7  # Learning rate
    discount_factor = 0.618

    MIN_REPLAY_SIZE = 1000
    if len(replay_memory) < MIN_REPLAY_SIZE:
        return

    batch_size = 64 * 2
    mini_batch = random.sample(replay_memory, batch_size)
    current_states = np.array(
        [encode_observation(transition[0], env.observation_space.shape) for transition in mini_batch])
    current_qs_list = model.predict(current_states)
    new_current_states = np.array(
        [encode_observation(transition[3], env.observation_space.shape) for transition in mini_batch])
    future_qs_list = target_model.predict(new_current_states)

    X = []
    Y = []
    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):
        if not done:
            max_future_q = reward + discount_factor * np.max(future_qs_list[index])
        else:
            max_future_q = reward

        current_qs = current_qs_list[index]
        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q

        X.append(encode_observation(observation, env.observation_space.shape))
        Y.append(current_qs)
    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)


def encode_observation(observation, n_dims):
    return observation


def agent(state_shape, action_shape):
    learning_rate = 0.001
    init = tf.keras.initializers.HeUniform()
    model = keras.Sequential()
    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))
    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))
    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))
    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate),
                  metrics=['accuracy'])
    return model


import gym
from gym import spaces


class HyperParameterTuning(gym.Env):
    """Custom Environment that follows gym interface"""

    # keeps track of the current episode
    current_episode = 0
    done = False

    def __init__(self, episode_length: int = 10):
        self.episide_length = episode_length
        super(HyperParameterTuning, self).__init__()
        self.action_space = spaces.Dict({
            # "gamma": spaces.Box(low=0, high=1, shape=(1,)),
            # "alpha": spaces.Box(low=0, high=1, shape=(1,)),
            "parent_popsize": spaces.Discrete(1),
            "child_popsize": spaces.Discrete(1),
            # "retry": spaces.Discrete(1),
        })
        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(1,))

    def step(self, action: dict):
        """Run one timestep of the environment's dynamics. When end of
        episode is reached, you are responsible for calling `reset()`
        to reset this environment's state.

        Accepts an action and returns a tuple (observation, reward, done, info).

        Args:
            action (object): an action provided by the agent

        Returns:
            observation (object): agent's observation of the current environment
            reward (float) : amount of reward returned after previous action
            done (bool): whether the episode has ended, in which case further step() calls will return undefined results
            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)
            """
        sres = SRES(
            cost_function=cost_fun,
            ngen=100,
            lb=[0.01] * 3,
            ub=[10] * 3,
            parent_popsize=action["parent_popsize"],
            child_popsize=action["child_popsize"],
            retry=100,
        )
        best_val = sres.fit(False)
        best_params = sres.getBestParameters()
        observation = best_params
        reward = np.log10(1 / best_val)  # we log just to make it a bit smaller
        if self.current_episode == self.episide_length:
            self.done = True
        else:
            self.current_episode += 1

        return observation, reward, self.done, ""

    def reset(self):
        """Resets the environment to an initial state and returns an initial
        observation.

        Note that this function should not reset the environment's random
        number generator(s); random variables in the environment's state should
        be sampled independently between multiple calls to `reset()`. In other
        words, each call of `reset()` should yield an environment suitable for
        a new episode, independent of previous episodes.

        Returns:
            observation (object): the initial observation.
        """
        self.done = False
        self.current_episode = 0
        return self

    def render(self, mode='human', close=False):
        # Render the environment to the screen
        pass


def main():
    epsilon = 1  # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start
    max_epsilon = 1  # You can't explore more than 100% of the time
    min_epsilon = 0.01  # At a minimum, we'll always explore 1% of the time
    decay = 0.01

    train_episodes = 300
    test_episodes = 100

    env = HyperParameterTuning(episode_length=10)

    # 1. Initialize the Target and Main models
    # Main Model (updated every step)
    model = agent(env.observation_space.shape, 2)
    # Target Model (updated every 100 steps)
    target_model = agent(env.observation_space.shape, 2)
    target_model.set_weights(model.get_weights())

    from collections import deque
    replay_memory = deque(maxlen=50_000)

    target_update_counter = 0

    # X = states, y = actions
    X = []
    y = []

    steps_to_update_target_model = 0

    for episode in range(train_episodes):
        total_training_rewards = 0
        observation = env.reset()
        done = False
        while not done:
            steps_to_update_target_model += 1

            random_number = np.random.rand()
            # 2. Explore using the Epsilon Greedy Exploration Strategy
            if random_number <= epsilon:
                # Explore
                action = env.action_space.sample()
            else:
                # Exploit best known action
                # model dims are (batch, env.observation_space.n)
                encoded = encode_observation(observation, env.observation_space.shape[0])
                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])
                predicted = model.predict(encoded_reshaped).flatten()
                action = np.argmax(predicted)
            new_observation, reward, done, info = env.step(action)
            replay_memory.append([observation, action, reward, new_observation, done])

            # 3. Update the Main Network using the Bellman Equation
            if steps_to_update_target_model % 4 == 0 or done:
                train(env, replay_memory, model, target_model, done)

            observation = new_observation
            total_training_rewards += reward

            if done:
                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(
                    total_training_rewards, episode, reward))
                total_training_rewards += 1

                if steps_to_update_target_model >= 100:
                    print('Copying main network weights to the target network weights')
                    target_model.set_weights(model.get_weights())
                    steps_to_update_target_model = 0
                break

        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)
    env.close()


if __name__ == '__main__':
    # need to stabilize the access voilations before trying this!
    main()

# sres.fit(printStats=True)

# objective_val = []
# for i in range(50):
#     objective_val.append(
#         sres.step(0.45, True)
#     )
#
# x_sim, y_sim, sel = get_data(
#     **dict(
#         zip(r.freeParameters(), sres.getBestParameters())
#     )
# )

# bug in tellurium when you try to access selection after named array has been sliced.

# print(x_sim, y_sim)
# print("sel", sel)
from matplotlib import cycler

# colours = ['c', 'm', 'y', 'k']
#
# fig, axes = plt.subplots(nrows=1, ncols=2)
# # axes[0].set_prop_cycle(custom_cycler)
# for i in range(len(sel)):
#     name = sel[i]
#     axes[0].plot(x_data, y_sim[:, i], label=f"{name}_sim", ls="--", color=colours[i])
#     axes[0].plot(x_data, y_data[:, i], label=f"{name}_exp", ls="None", marker='.', color=colours[i])
# axes[0].legend(loc=(-0.7, 0.1))
# axes[1].plot(range(len(objective_val)), objective_val, ls='-', marker='o')
# sns.despine(ax=axes[0], top=True, right=True)
# sns.despine(ax=axes[1], top=True, right=True)
# plt.savefig(os.path.join(os.path.dirname(__file__)), bbox_inches='tight', dpi=200)
