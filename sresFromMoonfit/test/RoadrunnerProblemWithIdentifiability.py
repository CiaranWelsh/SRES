import numpy as np
import tellurium as te
from sresFromMoonfit import SRES
from tellurium.roadrunner.extended_roadrunner import ExtendedRoadRunner

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from typing import List, Tuple, Dict
import os

mpl.use("TkAgg")


###################################################################################3
#   highjacking telluriums interface to roadrunner
#

def freeParameters(self):
    """
    Roadrunner models do not have a concept of "free" or "fixed"
    parameters (maybe it should?). Either way, we add a cheeky method
    to the tellurium interface to roadrunner to return the names
    of the parameters we want to fit
    """
    return ["k2", "k3"]


# assign, and now freeParameters is callable bound to ExtendedRoadrunner types
ExtendedRoadRunner.freeParameters = freeParameters

#####################################################################################3
#   model and data generation
#

# globally scoped model
ANTIMONY_STRING = """
   S1 -> S2; k1*S1;
   S2 -> S3; k2*S2;
   S3 -> S1; k3*S3;
   S1 = 1; S2 = 0; S3 = 0;
   k1 = 0.15; k2 = 0.45; k3 = 15;
"""
r = te.loada(ANTIMONY_STRING)


def get_data(**params) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    r.reset()
    for k, v in params.items():
        setattr(r, k, v)

    # Simulate "experimental" data
    m = r.simulate(0, 20, 21)
    x_data = m["time"]  # only time
    y_data = m[:, 1:]  # remove time
    return x_data, y_data, r.timeCourseSelections[1:]


x_data, y_data, selections = get_data(k1=0.15, k2=0.45, k3=0.05)

# Add random noise to synthetic data
for i in range(0, len(y_data)):
    t = np.random.normal(0, 0.01)
    y_data[i] = y_data[i] + t


##############################################################
#   Configure the optimization
#

@SRES.COST_FUNCTION_CALLBACK
def cost_fun(parameters, fitness, constraints):
    """
    Brief
    -----
    Compute difference between experimental dataset and model simulation with candidate parameters.
    This cost function is user defined and used as input to the main SRES algorithm. The input
    to this function is always [parameters, fitness and constraints]. You do not need to worry
    about generating candidate parameters as they are generated by the underlying algorithm. You do
    however have to worry about updating the fitness value, which you do like this:

        fitness.contents.value = calculated_cost

    Where calculated_cost is a float computed by your function. Note, that even though
    we haven't used the constraints argument in this cost function, we still need to pass it in
    as an input parameter.

    Details
    -------
    The underlying SRES C code requires as input a function pointer to a cost function
    that has the following signature:

        typedef void(*ESfcnFG)(double *, double *, double *);

    We can create a cost function in Python to pass to C by using the
    :py:class:`SRES.COST_FUNCTION_CALLBACK` decorator. Since the C end is
    expecting a function with three double pointer types, we must have
    as arguments to our cost function, three arguments.

    When coding the cost function, you need to remember that the types of
    parameter, fitness and constraints are ctypes pointers to double
    arrays in the case of the parameter and constraints argument and
    a pointer to a double in the case of fitness. To do computation
    with these types you need the value that the pointer points to, not
    the pointer. To get these, you use:
        >>> parameters.contents[0]
    In the case of pointer to a double array or
        >>> fitness.contents.value
    in the case of a pointer to a double.

    Args
    ----
    parameters: A list of candidate parameters with the same size as the
                dimensionality of your defined optimization problem.
    fitness:    This is the value that you must compute and assign.

    """
    x, y, sel = get_data(
        **dict(
            zip(r.freeParameters(), parameters.contents))
    )
    # note, you'll need two np.sums when >1 dataset
    cost = np.sum(np.sum((y - y_data) ** 2))

    # update fitness. This step is crucial and evey cost function
    # must have this line
    fitness.contents.value = cost


def plot(objective_val: List[float], y_sim: np.ndarray, sel: List[str]) -> None:
    colours = ['c', 'm', 'y', 'k']
    fig, axes = plt.subplots(nrows=1, ncols=2)
    print(type(objective_val), type(y_sim), type(sel))
    for i in range(len(sel)):
        name = sel[i]
        axes[0].plot(x_data, y_sim[:, i], label=f"{name}_sim", ls="--", color=colours[i])
        axes[0].plot(x_data, y_data[:, i], label=f"{name}_exp", ls="None", marker='.', color=colours[i])
    axes[0].legend(loc=(-0.7, 0.1))
    axes[0].set_ylabel("AU")
    axes[0].set_xlabel("Time step")
    axes[0].set_title("Best fits")

    axes[1].plot(range(len(objective_val)), np.log10(objective_val), ls='-', marker='o')
    axes[1].set_ylabel("log10 RSS")
    axes[1].set_xlabel("Generation")
    axes[1].set_title("Trace")
    sns.despine(ax=axes[0], top=True, right=True)
    sns.despine(ax=axes[1], top=True, right=True)
    plt.savefig(os.path.join(os.path.dirname(__file__)), bbox_inches='tight', dpi=200)


def do_estimation(ngen: int = 50) -> Tuple[List[float], np.ndarray, np.ndarray, List[str], Dict[str, float]]:
    sres = SRES(
        cost_function=cost_fun,
        ngen=ngen,
        lb=[0.01] * len(r.freeParameters()),
        ub=[100] * len(r.freeParameters()),
        parent_popsize=50,
        child_popsize=100,
        # retry=100,
        gamma=0.5
    )

    objective_val: List[float] = []
    for i in range(50):
        objective_val.append(
            sres.step(0.45, False)
        )

    best = dict(
        zip(r.freeParameters(), sres.getBestParameters())
    )
    x_sim, y_sim, sel = get_data(**best)
    return objective_val, x_sim, y_sim, sel, best


def plot_waterfall(best_values: List[float], stdevs: List[float]):
    best_values = sorted(best_values)
    fig, axes = plt.subplots(nrows=1, ncols=2)
    axes[0].plot(range(len(best_values)), best_values, marker='.', ls="None")
    axes[0].set_ylabel("RSS")
    axes[0].set_xlabel("Ranked best values")
    axes[0].set_title("Waterfall plot (n={})".format(len(best_values)))
    sns.despine(ax=axes[0], top=True, right=True)

    axes[1].plot(range(len(stdevs)), stdevs)
    axes[1].set_ylabel("stdev")
    axes[1].set_xlabel("Successive iterations")
    axes[1].set_title(f"Stdev of best RSS\n(n={len(stdevs)})")
    sns.despine(ax=axes[1], top=True, right=True)

    plt.savefig(os.path.join(
        os.path.dirname(__file__), "waterfall.png"), bbox_inches="tight", dpi=200
    )


def repeated_estimation(ngen: int = 50, n: int = 10):
    # do the first outside the loop to initialize the values
    objective_values_for_this_run, x_sim, y_sim, sel, best_parameters = do_estimation()
    best_values: List[float] = [objective_values_for_this_run[-1]]  # list to store the best values for each PE run
    best_objective_val = objective_values_for_this_run[-1]  # the best value from best_values
    best_y_sim = y_sim  # the best simulation data from the idx of best value
    std_devs = []  # For collecting running stdev

    for i in range(n - 1):
        # run an estimation
        objective_values_for_this_run, x_sim, y_sim, sel, best_parameters = do_estimation(ngen=ngen)
        best_values.append(objective_values_for_this_run[-1])
        # if the best value for the current run is better than all previous runs
        # we update the best* variables
        if objective_values_for_this_run[-1] <= min(best_values):
            best_objective_val = objective_values_for_this_run
            best_y_sim = y_sim
        current_stdev = np.std(best_values)
        std_devs.append(current_stdev)
        print("run:", i,
              ", best objective function value: ", objective_values_for_this_run[-1],
              ", current stdev of best values so far:", current_stdev,
              ", estimated parameter values: ", best_parameters
              )
    plot_waterfall(best_values, std_devs)
    plot(best_values, best_y_sim, sel)


def identifiability(best_estimated_parameters: Dict[str, float], num_points: int = 10):
    # we reload roadrunner here to prevent potential unintended consequences of modifying r inplace
    r = te.loada(ANTIMONY_STRING)

    # We make a copy of the original free parameters here
    # since we'll need to modify which parameters are estimated later
    # and put the original values back before each parameter
    original_free_parameters = r.freeParameters()

    # somewhere to store the results
    results = dict()

    for parameter in r.freeParameters():
        print("Running profile likelihood for parameter ", parameter)
        # get best estimated value for parameter
        if parameter not in best_estimated_parameters.keys():
            raise ValueError(f"Parameter {parameter} not in best estimated parameters: {best_estimated_parameters}")

        # reset our model back to the best parameter set
        for k, v in best_estimated_parameters.items():
            setattr(r, k, v)

        # we can use the interface from above but we need to update the
        # freeParameters method to contain only 'parameter'. In profile
        # likelihood, many parameter estimations are conducted, staring
        # from the best estimated parameter set. The difference is that
        # we fix 1 parameter (say k1) to a calculated value and reoptimize
        # all the others. Then we repeat, but with a different value of k1.

        def freeParameters(self):
            return [p for p in original_free_parameters if p != parameter]

        # tell roadrunner which parameters to estimate during reoptimization
        r.freeParameters = freeParameters

        # calculate the boundaries of the proile likelihood in logspace
        best_estimated_value = best_estimated_parameters[parameter]
        lb = best_estimated_value / 1000
        ub = best_estimated_value * 1000

        # these are the values that we will set fix parameter to
        x = np.logspace(np.log10(lb), np.log10(ub), num_points)

        best_rss_for_parameter_fixed_at_x = []  # for collecting the best rss values for each run
        list_of_best_params = []  # collect the best parameters for each run

        for fixed_parameter_value in x:
            print("scanning parameter value of: ", fixed_parameter_value)
            # 1) set the fixed parameter value
            setattr(r, parameter, fixed_parameter_value)
            # 2) repeat the parameter estimation.
            objective_values_for_this_run, x_sim, y_sim, sel, best_parameters = do_estimation(ngen=50)
            best_rss_for_parameter_fixed_at_x.append(objective_values_for_this_run[-1])
            list_of_best_params.append(best_parameters)
        results[parameter] = dict(
            x=x,
            list_of_best_params=list_of_best_params,
            best_rss_for_parameter_fixed_at_x=best_rss_for_parameter_fixed_at_x
        )
    return results


def plot_identifiability(best_estimated_parameters, identifiability_results, ncols=2):
    from matplotlib.gridspec import GridSpec
    n_parameters = len(identifiability_results)
    # calculate nrows needed for n_parameters in ncols
    nrows = int(np.ceil(n_parameters / ncols))
    fig = plt.figure(constrained_layout=True)
    # plt.ylabel("RSS value")
    # plt.xlabel("log10 x")
    grid = GridSpec(ncols=ncols, nrows=nrows, figure=fig)
    i = 0  # iter rows
    j = 0  # iter cols
    for parameter, identifiability_dct in identifiability_results.items():
        ax = fig.add_subplot(grid[i, j])
        ax.set_title(parameter)
        x_values = np.log10(identifiability_dct["x"])
        best_rss_for_parameter_fixed_at_x = identifiability_dct["best_rss_for_parameter_fixed_at_x"]
        ax.plot(x_values, best_rss_for_parameter_fixed_at_x, label=parameter, ls="--", marker=".")
        ax.vlines(
            best_estimated_parameters[parameter], ymin=0,
            ymax=max(best_rss_for_parameter_fixed_at_x),
            ls='--', color='red', alpha=0.5
        )
        sns.despine(ax=ax, top=True, right=True)

        # increment indices
        if i == nrows - 1:
            j += 1
            i = 0
        else:
            i += 1
    fname = os.path.join(os.path.dirname(__file__), "ProfileLikelihoods.png")
    plt.savefig(fname)


if __name__ == '__main__':
    # sns.set_context("talk")

    # number of generations for a single parameter estimation
    NGEN = 50
    # number of parameter estimations
    N = 50
    DO_SINGLE_ESTIMATION = False
    DO_MULTIPLE_ESTIMATIONS = False
    DO_IDENTIFIABILITY = True

    if DO_SINGLE_ESTIMATION:
        best, x_sim, y_sim, sel, best_parameters = do_estimation(ngen=NGEN)
        plot(best, y_sim, sel)

    if DO_MULTIPLE_ESTIMATIONS:
        repeated_estimation(ngen=NGEN, n=N)

    if DO_IDENTIFIABILITY:
        best_estimated_parameters = {'k1': 0.15, 'k2': 0.445, 'k3': 0.0500}
        results = identifiability(best_estimated_parameters, 10)
        plot_identifiability(best_estimated_parameters, results)

        # note: this dictionary was produced by running the identifiability function
        # Should really go in its own class to prettify this.
        id_results = {
            'k2':
                {'x': np.array([4.45000000e-04, 2.06550703e-03, 9.58723437e-03, 4.45000000e-02,
                                2.06550703e-01, 9.58723437e-01, 4.45000000e+00, 2.06550703e+01,
                                9.58723437e+01, 4.45000000e+02]),
                 'list_of_best_params': [
                     {'k2': 0.45146925396714777, 'k3': 0.049989778241204465},
                     {'k2': 0.45009427966643256, 'k3': 0.049999921237989915},
                     {'k2': 0.4519161370258281, 'k3': 0.04989003639454541},
                     {'k2': 0.4502140581978013, 'k3': 0.049938580264325516},
                     {'k2': 0.45037233066921484, 'k3': 0.05003693441081672},
                     {'k2': 0.4502503482071139, 'k3': 0.050028220342817814},
                     {'k2': 0.45011930741987216, 'k3': 0.05000899402470852},
                     {'k2': 0.4495081566617361, 'k3': 0.05006384664540234},
                     {'k2': 0.4490820377393757, 'k3': 0.049960975470068704},
                     {'k2': 0.4490820377393757, 'k3': 0.049960975470068704}],
                 'best_rss_for_parameter_fixed_at_x': [
                     0.005239832736358512, 0.005234532423929239,
                     0.005248465414973973, 0.005235521050571,
                     0.005234897161388682, 0.005234707343901575,
                     0.005234544675943229, 0.005236308348887274,
                     0.005236281741917724, 0.005236281741917724
                 ]},
            'k3': {
                'x': np.array(
                    [5.00000000e-05, 2.32079442e-04, 1.07721735e-03, 5.00000000e-03,
                     2.32079442e-02, 1.07721735e-01, 5.00000000e-01, 2.32079442e+00,
                     1.07721735e+01, 5.00000000e+01]),
                'list_of_best_params': [
                    {'k2': 0.44941033386607177, 'k3': 0.049932438729185756},
                    {'k2': 0.45003967288114965, 'k3': 0.049928648570264855},
                    {'k2': 0.5252738218075321, 'k3': 0.06809984035203648},
                    {'k2': 0.4481641784007903, 'k3': 0.04987907063114781},
                    {'k2': 0.44998171538470133, 'k3': 0.05000754216866573},
                    {'k2': 0.46105997671537313, 'k3': 0.05144126539560316},
                    {'k2': 0.46105997671537313, 'k3': 0.05144126539560316},
                    {'k2': 0.4501314403045926, 'k3': 0.049991173847504254},
                    {'k2': 0.6206146087188907, 'k3': 0.06743662593453403},
                    {'k2': 0.4499245212198626, 'k3': 0.04990208227082605}],
                'best_rss_for_parameter_fixed_at_x': [
                    0.005235619235639606, 0.005235515586747107,
                    0.0547498874667954, 0.0052252649042249235,
                    0.005234524812349352, 0.005677111598621542,
                    0.005677111598621542, 0.005234583627423296,
                    0.06837067432892588, 0.005236225216792437]
            }
        }

        # plot_identifiability(best_estimated_parameters, id_results)
